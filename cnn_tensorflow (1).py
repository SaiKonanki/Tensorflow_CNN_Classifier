# -*- coding: utf-8 -*-
"""CNN_Tensorflow.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12yNkJocvw-VnIfy4lh2PVrH4cBI7R7Yh
"""

import tensorflow as tf
import seaborn as sns
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.image as mpimg

print(tf.__version__)

#############################################################
# Obtaining MNIST data which is available as part of TF
#############################################################

mnist = tf.keras.datasets.mnist

# Splitting MNIST to train and test... x and y components
(x_train, y_train), (x_test, y_test) = mnist.load_data()

x_train.shape

pd.DataFrame(list(y_train), columns=['y_train'])['y_train'].value_counts()

# Using countplot to get the frequency distribution y (dependent) variable in the training dataset
sns.countplot(y_train)

# Checking if X has any nans in training data
np.isnan(x_train).any()

# Checking if X has any nans in testing data --- if yes, how do we treat them? dropna or fillna()?
np.isnan(x_test).any()

x_train.shape[0]

# Reshaping of the dataframe is required to pass it as input to model

input_shape = (28, 28, 1)

# Reshaping x_train
x_train=x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)
# Normalizing the data
x_train=x_train / 255.0

# Reshaping x_test
x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 1)
# Normalizing the data
x_test=x_test/255.0

# Currently our dependent dataframes are stored as categorical values like 1,2,3 etc. We would need to convert them using one-hot encoding.
y_train = tf.one_hot(y_train.astype(np.int32), depth=10)
y_test = tf.one_hot(y_test.astype(np.int32), depth=10)

y_train

# Visualize how the data looks like using both x and y
plt.imshow(x_train[1000][:,:,0])
print(y_train[1000])

### Building CNN model:

# CNN models are usually used for image classification purposes. Here, we are trying to use them with tensorflow.

# How many records should we pass through Neural network at once
batch_size = 64

# How many different classes are available in the testing data
num_classes = 10

# How many epochs? no. of times the data will have to pass through the neural network model
epochs = 5

### All of the above are hyperparameters that would require tuning (Google more and further details)

#########################################################################################################################
### Model Creation
#########################################################################################################################

# Model contains various layers stacked on top of each other. Output of one layer feeds into the input of next layer.
# Conv2D layers are convolutions which perform transformations on the image.
# Maxpool2D is a downsampling filter. It reduces a 2*2 matrix of the image to a single pixel value with the max value of the matrix.
# Filter aims to conserve the main features of the image while reducing the image
# Dropout is regularization filter. Randomly ignores 25% of the nodes. Reduces overfitting.
# Relu is the non linear activation function
# Flatten converts the tensors into a 1D vector
# Dense layers are an Artifical Neural Network. Last layer returns the probability that an image is in each class.
# As this is a multi class classification, model performance is measured using categorical cross entropy (what is good range? bad range?)

model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(32, (5,5), padding='same', activation='relu', input_shape=input_shape),
    tf.keras.layers.Conv2D(32, (5,5), padding='same', activation='relu'),
    tf.keras.layers.MaxPool2D(),
    tf.keras.layers.Dropout(0.25),
    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation='relu'),
    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation='relu'),
    tf.keras.layers.MaxPool2D(strides=(2,2)),
    tf.keras.layers.Dropout(0.25),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(num_classes, activation='softmax')
])

model.compile(optimizer=tf.keras.optimizers.RMSprop(epsilon=1e-08), loss='categorical_crossentropy', metrics=['acc'])

###########################################################################################################################
### Fit model to training data
###########################################################################################################################

# We need to train the model until we achieve a certain level of accuracy. We need not continue to train the model if
# required accuracy is reached especially if computational and time resources are limited

# Below callback function enables function to stop training if 99.5% accuracy is achieved

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('acc')>0.995):
      print("\nReached 99.5% accuracy so cancelling training!")
      self.model.stop_training = True

callbacks = myCallback()

############################################################################################################################
### Building the model on 90% of dataset and validating using 10% of the dataset to avoid overfitting
############################################################################################################################

history = model.fit(x_train, y_train,
                    batch_size=batch_size,
                    epochs=epochs,
                    validation_split=0.1,
                    callbacks=[callbacks])

######################################
### Model Evaluation
######################################

# We need to monitor accuracy and loss on training and validation sets

fig, ax = plt.subplots(2,1)
ax[0].plot(history.history['loss'], color='b', label="Training Loss")
ax[0].plot(history.history['val_loss'], color='r', label="Validation Loss",axes =ax[0])
legend = ax[0].legend(loc='best', shadow=True)

ax[1].plot(history.history['acc'], color='b', label="Training Accuracy")
ax[1].plot(history.history['val_acc'], color='r',label="Validation Accuracy")
legend = ax[1].legend(loc='best', shadow=True)

# Make predictions and get accuracy metrics for test dataset

test_loss, test_acc = model.evaluate(x_test, y_test)

# Below step provides confusion matrix using tensorflow

# Predict the values from the testing dataset
Y_pred = model.predict(x_test)
# Convert predictions classes to one hot vectors
Y_pred_classes = np.argmax(Y_pred,axis = 1)
# Convert testing observations to one hot vectors
Y_true = np.argmax(y_test,axis = 1)
# compute the confusion matrix
confusion_mtx = tf.math.confusion_matrix(Y_true, Y_pred_classes)

# Below confusion martrix shows what numbers are misclassified as other numbers

plt.figure(figsize=(10, 8))
sns.heatmap(confusion_mtx, annot=True, fmt='g')